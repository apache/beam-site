

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>apache_beam.ml.inference.huggingface_inference &mdash; Apache Beam 2.54.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> Apache Beam
          

          
          </a>

          
            
            
              <div class="version">
                2.54.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.coders.html">apache_beam.coders package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.dataframe.html">apache_beam.dataframe package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.io.html">apache_beam.io package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.metrics.html">apache_beam.metrics package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.ml.html">apache_beam.ml package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.options.html">apache_beam.options package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.portability.html">apache_beam.portability package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.runners.html">apache_beam.runners package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.testing.html">apache_beam.testing package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.transforms.html">apache_beam.transforms package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.typehints.html">apache_beam.typehints package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.utils.html">apache_beam.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.yaml.html">apache_beam.yaml package</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.error.html">apache_beam.error module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.pipeline.html">apache_beam.pipeline module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../apache_beam.pvalue.html">apache_beam.pvalue module</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Apache Beam</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>apache_beam.ml.inference.huggingface_inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for apache_beam.ml.inference.huggingface_inference</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License. You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>

<span class="c1"># pytype: skip-file</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Iterable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">apache_beam.ml.inference</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">apache_beam.ml.inference.base</span> <span class="kn">import</span> <span class="n">ModelHandler</span>
<span class="kn">from</span> <span class="nn">apache_beam.ml.inference.base</span> <span class="kn">import</span> <span class="n">PredictionResult</span>
<span class="kn">from</span> <span class="nn">apache_beam.ml.inference.pytorch_inference</span> <span class="kn">import</span> <span class="n">_convert_to_device</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFAutoModel</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">_LOGGER</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;HuggingFaceModelHandlerTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HuggingFaceModelHandlerKeyedTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HuggingFacePipelineModelHandler&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">TensorInferenceFn</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span>
    <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
    <span class="nb">str</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
<span class="p">],</span>
                             <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">],</span>
                             <span class="p">]</span>

<span class="n">KeyedTensorInferenceFn</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span>
    <span class="n">Sequence</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span>
    <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
    <span class="nb">str</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="p">],</span>
                                  <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]]</span>

<span class="n">PipelineInferenceFn</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[</span>
    <span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span>
    <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]]</span>


<span class="k">class</span> <span class="nc">PipelineTask</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Enum</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  PipelineTask defines all the tasks supported by the Hugging Face Pipelines</span>
<span class="sd">  listed at https://huggingface.co/docs/transformers/main_classes/pipelines.</span>
<span class="sd">  Only these tasks can be passed to HuggingFacePipelineModelHandler.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">AudioClassification</span> <span class="o">=</span> <span class="s1">&#39;audio-classification&#39;</span>
  <span class="n">AutomaticSpeechRecognition</span> <span class="o">=</span> <span class="s1">&#39;automatic-speech-recognition&#39;</span>
  <span class="n">Conversational</span> <span class="o">=</span> <span class="s1">&#39;conversational&#39;</span>
  <span class="n">DepthEstimation</span> <span class="o">=</span> <span class="s1">&#39;depth-estimation&#39;</span>
  <span class="n">DocumentQuestionAnswering</span> <span class="o">=</span> <span class="s1">&#39;document-question-answering&#39;</span>
  <span class="n">FeatureExtraction</span> <span class="o">=</span> <span class="s1">&#39;feature-extraction&#39;</span>
  <span class="n">FillMask</span> <span class="o">=</span> <span class="s1">&#39;fill-mask&#39;</span>
  <span class="n">ImageClassification</span> <span class="o">=</span> <span class="s1">&#39;image-classification&#39;</span>
  <span class="n">ImageSegmentation</span> <span class="o">=</span> <span class="s1">&#39;image-segmentation&#39;</span>
  <span class="n">ImageToText</span> <span class="o">=</span> <span class="s1">&#39;image-to-text&#39;</span>
  <span class="n">MaskGeneration</span> <span class="o">=</span> <span class="s1">&#39;mask-generation&#39;</span>
  <span class="n">NER</span> <span class="o">=</span> <span class="s1">&#39;ner&#39;</span>
  <span class="n">ObjectDetection</span> <span class="o">=</span> <span class="s1">&#39;object-detection&#39;</span>
  <span class="n">QuestionAnswering</span> <span class="o">=</span> <span class="s1">&#39;question-answering&#39;</span>
  <span class="n">SentimentAnalysis</span> <span class="o">=</span> <span class="s1">&#39;sentiment-analysis&#39;</span>
  <span class="n">Summarization</span> <span class="o">=</span> <span class="s1">&#39;summarization&#39;</span>
  <span class="n">TableQuestionAnswering</span> <span class="o">=</span> <span class="s1">&#39;table-question-answering&#39;</span>
  <span class="n">TextClassification</span> <span class="o">=</span> <span class="s1">&#39;text-classification&#39;</span>
  <span class="n">TextGeneration</span> <span class="o">=</span> <span class="s1">&#39;text-generation&#39;</span>
  <span class="n">Text2TextGeneration</span> <span class="o">=</span> <span class="s1">&#39;text2text-generation&#39;</span>
  <span class="n">TextToAudio</span> <span class="o">=</span> <span class="s1">&#39;text-to-audio&#39;</span>
  <span class="n">TokenClassification</span> <span class="o">=</span> <span class="s1">&#39;token-classification&#39;</span>
  <span class="n">Translation</span> <span class="o">=</span> <span class="s1">&#39;translation&#39;</span>
  <span class="n">VideoClassification</span> <span class="o">=</span> <span class="s1">&#39;video-classification&#39;</span>
  <span class="n">VisualQuestionAnswering</span> <span class="o">=</span> <span class="s1">&#39;visual-question-answering&#39;</span>
  <span class="n">VQA</span> <span class="o">=</span> <span class="s1">&#39;vqa&#39;</span>
  <span class="n">ZeroShotAudioClassification</span> <span class="o">=</span> <span class="s1">&#39;zero-shot-audio-classification&#39;</span>
  <span class="n">ZeroShotClassification</span> <span class="o">=</span> <span class="s1">&#39;zero-shot-classification&#39;</span>
  <span class="n">ZeroShotImageClassification</span> <span class="o">=</span> <span class="s1">&#39;zero-shot-image-classification&#39;</span>
  <span class="n">ZeroShotObjectDetection</span> <span class="o">=</span> <span class="s1">&#39;zero-shot-object-detection&#39;</span>
  <span class="n">Translation_XX_to_YY</span> <span class="o">=</span> <span class="s1">&#39;translation_XX_to_YY&#39;</span>


<span class="k">def</span> <span class="nf">_validate_constructor_args</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">model_class</span><span class="p">):</span>
  <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
      <span class="s2">&quot;Please provide both model class and model uri to load the model.&quot;</span>
      <span class="s2">&quot;Got params as model_uri=</span><span class="si">{model_uri}</span><span class="s2"> and &quot;</span>
      <span class="s2">&quot;model_class=</span><span class="si">{model_class}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">model_uri</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">model_class</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">model_class</span><span class="o">=</span><span class="n">model_class</span><span class="p">))</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">model_uri</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">model_class</span><span class="o">=</span><span class="n">model_class</span><span class="p">))</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">model_class</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">model_class</span><span class="o">=</span><span class="n">model_class</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">no_gpu_available_warning</span><span class="p">():</span>
  <span class="n">_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
      <span class="s2">&quot;HuggingFaceModelHandler specified a &#39;GPU&#39; device, &quot;</span>
      <span class="s2">&quot;but GPUs are not available. Switching to CPU.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_gpu_available_torch</span><span class="p">():</span>
  <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">no_gpu_available_warning</span><span class="p">()</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">get_device_torch</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="n">is_gpu_available_torch</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_gpu_available_tensorflow</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
  <span class="n">gpu_devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpu_devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">no_gpu_available_warning</span><span class="p">()</span>
    <span class="k">return</span> <span class="kc">False</span>
  <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_validate_constructor_args_hf_pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">task</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">model</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s1">&#39;Please provide either task or model to the &#39;</span>
        <span class="s1">&#39;HuggingFacePipelineModelHandler. If the model already defines the &#39;</span>
        <span class="s1">&#39;task, no need to specify the task.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_run_inference_torch_keyed_tensor</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">AutoModel</span><span class="p">,</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">inference_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">get_device_torch</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">key_to_tensor_list</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="c1"># torch.no_grad() mitigates GPU memory issues</span>
  <span class="c1"># https://github.com/apache/beam/issues/22811</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">example</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">key_to_tensor_list</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="n">key_to_batched_tensors</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_to_tensor_list</span><span class="p">:</span>
      <span class="n">batched_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">key_to_tensor_list</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
      <span class="n">batched_tensors</span> <span class="o">=</span> <span class="n">_convert_to_device</span><span class="p">(</span><span class="n">batched_tensors</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
      <span class="n">key_to_batched_tensors</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">batched_tensors</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">key_to_batched_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">inference_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">_convert_to_result</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_run_inference_tensorflow_keyed_tensor</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">TFAutoModel</span><span class="p">,</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">inference_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
    <span class="n">is_gpu_available_tensorflow</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">key_to_tensor_list</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">example</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">key_to_tensor_list</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">key_to_batched_tensors</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_to_tensor_list</span><span class="p">:</span>
    <span class="n">batched_tensors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">key_to_tensor_list</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">key_to_batched_tensors</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">batched_tensors</span>
  <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">key_to_batched_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">inference_args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">_convert_to_result</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>


<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor">[docs]</a><span class="k">class</span> <span class="nc">HuggingFaceModelHandlerKeyedTensor</span><span class="p">(</span><span class="n">ModelHandler</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span>
                                                           <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                                                                 <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
                                                      <span class="n">PredictionResult</span><span class="p">,</span>
                                                      <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span>
                                                            <span class="n">TFAutoModel</span><span class="p">]]):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">model_uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
      <span class="n">model_class</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
      <span class="n">framework</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
      <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span><span class="p">,</span>
      <span class="o">*</span><span class="p">,</span>
      <span class="n">inference_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">load_model_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">min_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">max_batch_duration_secs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">large_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
      <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of the ModelHandler interface for HuggingFace with</span>
<span class="sd">    Keyed Tensors for PyTorch/Tensorflow backend.</span>

<span class="sd">    Example Usage model::</span>
<span class="sd">      pcoll | RunInference(HuggingFaceModelHandlerKeyedTensor(</span>
<span class="sd">        model_uri=&quot;bert-base-uncased&quot;, model_class=AutoModelForMaskedLM,</span>
<span class="sd">        framework=&#39;pt&#39;))</span>

<span class="sd">    Args:</span>
<span class="sd">      model_uri (str): path to the pretrained model on the hugging face</span>
<span class="sd">        models hub.</span>
<span class="sd">      model_class: model class to load the repository from model_uri.</span>
<span class="sd">      framework (str): Framework to use for the model. &#39;tf&#39; for TensorFlow and</span>
<span class="sd">        &#39;pt&#39; for PyTorch.</span>
<span class="sd">      device: For torch tensors, specify device on which you wish to</span>
<span class="sd">        run the model. Defaults to CPU.</span>
<span class="sd">      inference_fn: the inference function to use during RunInference.</span>
<span class="sd">        Default is _run_inference_torch_keyed_tensor or</span>
<span class="sd">        _run_inference_tensorflow_keyed_tensor depending on the input type.</span>
<span class="sd">      load_model_args (Dict[str, Any]): (Optional) Keyword arguments to provide</span>
<span class="sd">        load options while loading models from Hugging Face Hub.</span>
<span class="sd">        Defaults to None.</span>
<span class="sd">      min_batch_size: the minimum batch size to use when batching inputs.</span>
<span class="sd">      max_batch_size: the maximum batch size to use when batching inputs.</span>
<span class="sd">      max_batch_duration_secs: the maximum amount of time to buffer a batch</span>
<span class="sd">        before emitting; used in streaming contexts.</span>
<span class="sd">      large_model: set to true if your model is large enough to run into</span>
<span class="sd">        memory pressure if you load multiple copies. Given a model that</span>
<span class="sd">        consumes N memory and a machine with W cores and M memory, you should</span>
<span class="sd">        set this to True if N*W &gt; M.</span>
<span class="sd">      kwargs: &#39;env_vars&#39; can be used to set environment variables</span>
<span class="sd">        before loading the model.</span>

<span class="sd">    **Supported Versions:** HuggingFaceModelHandler supports</span>
<span class="sd">    transformers&gt;=4.18.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span> <span class="o">=</span> <span class="n">model_uri</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_class</span> <span class="o">=</span> <span class="n">model_class</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span> <span class="o">=</span> <span class="n">inference_fn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_config_args</span> <span class="o">=</span> <span class="n">load_model_args</span> <span class="k">if</span> <span class="n">load_model_args</span> <span class="k">else</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_env_vars</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;env_vars&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="k">if</span> <span class="n">min_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;min_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_batch_size</span>
    <span class="k">if</span> <span class="n">max_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;max_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_batch_size</span>
    <span class="k">if</span> <span class="n">max_batch_duration_secs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;max_batch_duration_secs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_batch_duration_secs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_large_model</span> <span class="o">=</span> <span class="n">large_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">=</span> <span class="n">framework</span>

    <span class="n">_validate_constructor_args</span><span class="p">(</span>
        <span class="n">model_uri</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">,</span> <span class="n">model_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_class</span><span class="p">)</span>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.load_model"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.load_model">[docs]</a>  <span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads and initializes the model for processing.&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_config_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">==</span> <span class="s1">&#39;pt&#39;</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="n">is_gpu_available_torch</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
      <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;requires_grad_&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.run_inference"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.run_inference">[docs]</a>  <span class="k">def</span> <span class="nf">run_inference</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span>
      <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
      <span class="n">inference_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs inferences on a batch of Keyed Tensors and returns an Iterable of</span>
<span class="sd">    Tensors Predictions.</span>

<span class="sd">    This method stacks the list of Tensors in a vectorized format to optimize</span>
<span class="sd">    the inference call.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch: A sequence of Keyed Tensors. These Tensors should be batchable,</span>
<span class="sd">        as this method will call `tf.stack()`/`torch.stack()` and pass in</span>
<span class="sd">        batched Tensors with dimensions (batch_size, n_features, etc.) into</span>
<span class="sd">        the model&#39;s predict() function.</span>
<span class="sd">      model: A Tensorflow/PyTorch model.</span>
<span class="sd">      inference_args: Non-batchable arguments required as inputs to the</span>
<span class="sd">        model&#39;s inference function. Unlike Tensors in `batch`,</span>
<span class="sd">        these parameters will not be dynamically batched.</span>
<span class="sd">    Returns:</span>
<span class="sd">      An Iterable of type PredictionResult.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inference_args</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">inference_args</span> <span class="k">else</span> <span class="n">inference_args</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span><span class="p">(</span>
          <span class="n">batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_run_inference_tensorflow_keyed_tensor</span><span class="p">(</span>
          <span class="n">batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_run_inference_torch_keyed_tensor</span><span class="p">(</span>
          <span class="n">batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">)</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.update_model_path"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.update_model_path">[docs]</a>  <span class="k">def</span> <span class="nf">update_model_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span> <span class="o">=</span> <span class="n">model_path</span> <span class="k">if</span> <span class="n">model_path</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.get_num_bytes"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.get_num_bytes">[docs]</a>  <span class="k">def</span> <span class="nf">get_num_bytes</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">      The number of bytes of data for the Tensors batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">element</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span>
          <span class="p">(</span><span class="n">el</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">tensor</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.batch_elements_kwargs"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.batch_elements_kwargs">[docs]</a>  <span class="k">def</span> <span class="nf">batch_elements_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.share_model_across_processes"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.share_model_across_processes">[docs]</a>  <span class="k">def</span> <span class="nf">share_model_across_processes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_large_model</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerKeyedTensor.get_metrics_namespace"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerKeyedTensor.get_metrics_namespace">[docs]</a>  <span class="k">def</span> <span class="nf">get_metrics_namespace</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        A namespace for metrics collected by the RunInference transform.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;BeamML_HuggingFaceModelHandler_KeyedTensor&quot;</span></div></div>


<span class="k">def</span> <span class="nf">_default_inference_fn_torch</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">inference_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">get_device_torch</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="c1"># torch.no_grad() mitigates GPU memory issues</span>
  <span class="c1"># https://github.com/apache/beam/issues/22811</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">batched_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">batched_tensors</span> <span class="o">=</span> <span class="n">_convert_to_device</span><span class="p">(</span><span class="n">batched_tensors</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batched_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">inference_args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">_convert_to_result</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_default_inference_fn_tensorflow</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">inference_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
    <span class="n">is_gpu_available_tensorflow</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">batched_tensors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batched_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">inference_args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">_convert_to_result</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>


<div class="viewcode-block" id="HuggingFaceModelHandlerTensor"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor">[docs]</a><span class="k">class</span> <span class="nc">HuggingFaceModelHandlerTensor</span><span class="p">(</span><span class="n">ModelHandler</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                                 <span class="n">PredictionResult</span><span class="p">,</span>
                                                 <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span>
                                                       <span class="n">TFAutoModel</span><span class="p">]]):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">model_uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
      <span class="n">model_class</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
      <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span><span class="p">,</span>
      <span class="o">*</span><span class="p">,</span>
      <span class="n">inference_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">load_model_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">min_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">max_batch_duration_secs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">large_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
      <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of the ModelHandler interface for HuggingFace with</span>
<span class="sd">    Tensors for PyTorch/Tensorflow backend.</span>

<span class="sd">    Depending on the type of tensors, the model framework is determined</span>
<span class="sd">    automatically.</span>

<span class="sd">    Example Usage model:</span>
<span class="sd">      pcoll | RunInference(HuggingFaceModelHandlerTensor(</span>
<span class="sd">        model_uri=&quot;bert-base-uncased&quot;, model_class=AutoModelForMaskedLM))</span>

<span class="sd">    Args:</span>
<span class="sd">      model_uri (str): path to the pretrained model on the hugging face</span>
<span class="sd">        models hub.</span>
<span class="sd">      model_class: model class to load the repository from model_uri.</span>
<span class="sd">      device: For torch tensors, specify device on which you wish to</span>
<span class="sd">        run the model. Defaults to CPU.</span>
<span class="sd">      inference_fn: the inference function to use during RunInference.</span>
<span class="sd">        Default is _run_inference_torch_keyed_tensor or</span>
<span class="sd">        _run_inference_tensorflow_keyed_tensor depending on the input type.</span>
<span class="sd">      load_model_args (Dict[str, Any]): (Optional) keyword arguments to provide</span>
<span class="sd">        load options while loading models from Hugging Face Hub.</span>
<span class="sd">        Defaults to None.</span>
<span class="sd">      min_batch_size: the minimum batch size to use when batching inputs.</span>
<span class="sd">      max_batch_size: the maximum batch size to use when batching inputs.</span>
<span class="sd">      max_batch_duration_secs: the maximum amount of time to buffer a batch</span>
<span class="sd">        before emitting; used in streaming contexts.</span>
<span class="sd">      large_model: set to true if your model is large enough to run into</span>
<span class="sd">        memory pressure if you load multiple copies. Given a model that</span>
<span class="sd">        consumes N memory and a machine with W cores and M memory, you should</span>
<span class="sd">        set this to True if N*W &gt; M.</span>
<span class="sd">      kwargs: &#39;env_vars&#39; can be used to set environment variables</span>
<span class="sd">        before loading the model.</span>

<span class="sd">    **Supported Versions:** HuggingFaceModelHandler supports</span>
<span class="sd">    transformers&gt;=4.18.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span> <span class="o">=</span> <span class="n">model_uri</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_class</span> <span class="o">=</span> <span class="n">model_class</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span> <span class="o">=</span> <span class="n">inference_fn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_config_args</span> <span class="o">=</span> <span class="n">load_model_args</span> <span class="k">if</span> <span class="n">load_model_args</span> <span class="k">else</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_env_vars</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;env_vars&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="k">if</span> <span class="n">min_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;min_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_batch_size</span>
    <span class="k">if</span> <span class="n">max_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;max_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_batch_size</span>
    <span class="k">if</span> <span class="n">max_batch_duration_secs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;max_batch_duration_secs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_batch_duration_secs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_large_model</span> <span class="o">=</span> <span class="n">large_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

    <span class="n">_validate_constructor_args</span><span class="p">(</span>
        <span class="n">model_uri</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">,</span> <span class="n">model_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_class</span><span class="p">)</span>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.load_model"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.load_model">[docs]</a>  <span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads and initializes the model for processing.&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_config_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;requires_grad_&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)):</span>
      <span class="n">model</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.run_inference"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.run_inference">[docs]</a>  <span class="k">def</span> <span class="nf">run_inference</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
      <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AutoModel</span><span class="p">,</span> <span class="n">TFAutoModel</span><span class="p">],</span>
      <span class="n">inference_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs inferences on a batch of Tensors and returns an Iterable of</span>
<span class="sd">    Tensors Predictions.</span>

<span class="sd">    This method stacks the list of Tensors in a vectorized format to optimize</span>
<span class="sd">    the inference call.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch: A sequence of Tensors. These Tensors should be batchable, as</span>
<span class="sd">        this method will call `tf.stack()`/`torch.stack()` and pass in</span>
<span class="sd">        batched Tensors with dimensions (batch_size, n_features, etc.)</span>
<span class="sd">        into the model&#39;s predict() function.</span>
<span class="sd">      model: A Tensorflow/PyTorch model.</span>
<span class="sd">      inference_args (Dict[str, Any]): Non-batchable arguments required as</span>
<span class="sd">        inputs to the model&#39;s inference function. Unlike Tensors in `batch`,</span>
<span class="sd">        these parameters will not be dynamically batched.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An Iterable of type PredictionResult.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inference_args</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">inference_args</span> <span class="k">else</span> <span class="n">inference_args</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">=</span> <span class="s2">&quot;tf&quot;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span>

    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span>
        <span class="n">is_gpu_available_torch</span><span class="p">()):</span>
      <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span><span class="p">(</span>
          <span class="n">batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_default_inference_fn_tensorflow</span><span class="p">(</span>
          <span class="n">batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_default_inference_fn_torch</span><span class="p">(</span>
          <span class="n">batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span><span class="p">)</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.update_model_path"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.update_model_path">[docs]</a>  <span class="k">def</span> <span class="nf">update_model_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span> <span class="o">=</span> <span class="n">model_path</span> <span class="k">if</span> <span class="n">model_path</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_uri</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.get_num_bytes"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.get_num_bytes">[docs]</a>  <span class="k">def</span> <span class="nf">get_num_bytes</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">      The number of bytes of data for the Tensors batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">element</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span>
          <span class="p">(</span><span class="n">el</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">tensor</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.batch_elements_kwargs"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.batch_elements_kwargs">[docs]</a>  <span class="k">def</span> <span class="nf">batch_elements_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.share_model_across_processes"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.share_model_across_processes">[docs]</a>  <span class="k">def</span> <span class="nf">share_model_across_processes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_large_model</span></div>

<div class="viewcode-block" id="HuggingFaceModelHandlerTensor.get_metrics_namespace"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFaceModelHandlerTensor.get_metrics_namespace">[docs]</a>  <span class="k">def</span> <span class="nf">get_metrics_namespace</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">       A namespace for metrics collected by the RunInference transform.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;BeamML_HuggingFaceModelHandler_Tensor&#39;</span></div></div>


<span class="k">def</span> <span class="nf">_convert_to_result</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Iterable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">]],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
  <span class="k">return</span> <span class="p">[</span>
      <span class="n">PredictionResult</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">[</span><span class="n">predictions</span><span class="p">])</span>
  <span class="p">]</span>


<span class="k">def</span> <span class="nf">_default_pipeline_inference_fn</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
  <span class="n">predicitons</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">inference_args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">predicitons</span>


<div class="viewcode-block" id="HuggingFacePipelineModelHandler"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler">[docs]</a><span class="k">class</span> <span class="nc">HuggingFacePipelineModelHandler</span><span class="p">(</span><span class="n">ModelHandler</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span>
                                                   <span class="n">PredictionResult</span><span class="p">,</span>
                                                   <span class="n">Pipeline</span><span class="p">]):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">task</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PipelineTask</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
      <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
      <span class="o">*</span><span class="p">,</span>
      <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">inference_fn</span><span class="p">:</span> <span class="n">PipelineInferenceFn</span> <span class="o">=</span> <span class="n">_default_pipeline_inference_fn</span><span class="p">,</span>
      <span class="n">load_pipeline_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">min_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">max_batch_duration_secs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">large_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
      <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of the ModelHandler interface for Hugging Face Pipelines.</span>

<span class="sd">    Example Usage model::</span>
<span class="sd">      pcoll | RunInference(HuggingFacePipelineModelHandler(</span>
<span class="sd">        task=&quot;fill-mask&quot;))</span>

<span class="sd">    Args:</span>
<span class="sd">      task (str or enum.Enum): task supported by HuggingFace Pipelines.</span>
<span class="sd">        Accepts a string task or an enum.Enum from PipelineTask.</span>
<span class="sd">      model (str): path to the pretrained *model-id* on Hugging Face Models Hub</span>
<span class="sd">        to use custom model for the chosen task. If the `model` already defines</span>
<span class="sd">        the task then no need to specify the `task` parameter.</span>
<span class="sd">        Use the *model-id* string instead of an actual model here.</span>
<span class="sd">        Model-specific kwargs for `from_pretrained(..., **model_kwargs)` can be</span>
<span class="sd">        specified with `model_kwargs` using `load_pipeline_args`.</span>

<span class="sd">        Example Usage::</span>
<span class="sd">          model_handler = HuggingFacePipelineModelHandler(</span>
<span class="sd">            task=&quot;text-generation&quot;, model=&quot;meta-llama/Llama-2-7b-hf&quot;,</span>
<span class="sd">            load_pipeline_args={&#39;model_kwargs&#39;:{&#39;quantization_map&#39;:config}})</span>

<span class="sd">      device (str): the device (`&quot;CPU&quot;` or `&quot;GPU&quot;`) on which you wish to run</span>
<span class="sd">        the pipeline. Defaults to GPU. If GPU is not available then it falls</span>
<span class="sd">        back to CPU. You can also use advanced option like `device_map` with</span>
<span class="sd">        key-value pair as you would do in the usual Hugging Face pipeline using</span>
<span class="sd">        `load_pipeline_args`. Ex: load_pipeline_args={&#39;device_map&#39;:auto}).</span>
<span class="sd">      inference_fn: the inference function to use during RunInference.</span>
<span class="sd">        Default is _default_pipeline_inference_fn.</span>
<span class="sd">      load_pipeline_args (Dict[str, Any]): keyword arguments to provide load</span>
<span class="sd">        options while loading pipelines from Hugging Face. Defaults to None.</span>
<span class="sd">      min_batch_size: the minimum batch size to use when batching inputs.</span>
<span class="sd">      max_batch_size: the maximum batch size to use when batching inputs.</span>
<span class="sd">      max_batch_duration_secs: the maximum amount of time to buffer a batch</span>
<span class="sd">        before emitting; used in streaming contexts.</span>
<span class="sd">      large_model: set to true if your model is large enough to run into</span>
<span class="sd">        memory pressure if you load multiple copies. Given a model that</span>
<span class="sd">        consumes N memory and a machine with W cores and M memory, you should</span>
<span class="sd">        set this to True if N*W &gt; M.</span>
<span class="sd">      kwargs: &#39;env_vars&#39; can be used to set environment variables</span>
<span class="sd">        before loading the model.</span>

<span class="sd">    **Supported Versions:** HuggingFacePipelineModelHandler supports</span>
<span class="sd">    transformers&gt;=4.18.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_task</span> <span class="o">=</span> <span class="n">task</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span> <span class="o">=</span> <span class="n">inference_fn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_load_pipeline_args</span> <span class="o">=</span> <span class="n">load_pipeline_args</span> <span class="k">if</span> <span class="n">load_pipeline_args</span> <span class="k">else</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_framework</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_env_vars</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;env_vars&#39;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="k">if</span> <span class="n">min_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s1">&#39;min_batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_batch_size</span>
    <span class="k">if</span> <span class="n">max_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s1">&#39;max_batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_batch_size</span>
    <span class="k">if</span> <span class="n">max_batch_duration_secs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span><span class="p">[</span><span class="s2">&quot;max_batch_duration_secs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_batch_duration_secs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_large_model</span> <span class="o">=</span> <span class="n">large_model</span>

    <span class="c1"># Check if the device is specified twice. If true then the device parameter</span>
    <span class="c1"># of model handler is overridden.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_deduplicate_device_value</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">_validate_constructor_args_hf_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_task</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_deduplicate_device_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">if</span> <span class="n">device</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">current_device</span> <span class="ow">and</span> <span class="n">current_device</span> <span class="o">!=</span> <span class="s1">&#39;CPU&#39;</span> <span class="ow">and</span> <span class="n">current_device</span> <span class="o">!=</span> <span class="s1">&#39;GPU&#39;</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Invalid device value: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">. Please specify &quot;</span>
          <span class="s2">&quot;either CPU or GPU. Defaults to GPU if no value &quot;</span>
          <span class="s2">&quot;is provided.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;device&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_pipeline_args</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">current_device</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_pipeline_args</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_gpu_available_torch</span><span class="p">():</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_load_pipeline_args</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cuda:1&#39;</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
              <span class="s2">&quot;HuggingFaceModelHandler specified a &#39;GPU&#39; device, &quot;</span>
              <span class="s2">&quot;but GPUs are not available. Switching to CPU.&quot;</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_load_pipeline_args</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">current_device</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;`device` specified in `load_pipeline_args`. `device` &#39;</span>
            <span class="s1">&#39;parameter for HuggingFacePipelineModelHandler will be ignored.&#39;</span><span class="p">)</span>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.load_model"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.load_model">[docs]</a>  <span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads and initializes the pipeline for processing.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pipeline</span><span class="p">(</span>
        <span class="n">task</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_load_pipeline_args</span><span class="p">)</span></div>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.run_inference"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.run_inference">[docs]</a>  <span class="k">def</span> <span class="nf">run_inference</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
      <span class="n">pipeline</span><span class="p">:</span> <span class="n">Pipeline</span><span class="p">,</span>
      <span class="n">inference_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">PredictionResult</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs inferences on a batch of examples passed as a string resource.</span>
<span class="sd">    These can either be string sentences, or string path to images or</span>
<span class="sd">    audio files.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch: A sequence of strings resources.</span>
<span class="sd">      pipeline: A Hugging Face Pipeline.</span>
<span class="sd">      inference_args: Non-batchable arguments required as inputs to the model&#39;s</span>
<span class="sd">        inference function.</span>
<span class="sd">    Returns:</span>
<span class="sd">      An Iterable of type PredictionResult.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inference_args</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">inference_args</span> <span class="k">else</span> <span class="n">inference_args</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">inference_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_convert_to_result</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span></div>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.update_model_path"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.update_model_path">[docs]</a>  <span class="k">def</span> <span class="nf">update_model_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the pretrained model used by the Hugging Face Pipeline task.</span>
<span class="sd">    Make sure that the new model does the same task as initial model.</span>

<span class="sd">    Args:</span>
<span class="sd">      model_path (str): (Optional) Path to the new trained model</span>
<span class="sd">        from Hugging Face. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model_path</span> <span class="k">if</span> <span class="n">model_path</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span></div>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.get_num_bytes"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.get_num_bytes">[docs]</a>  <span class="k">def</span> <span class="nf">get_num_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">      The number of bytes of input batch elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">element</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span></div>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.batch_elements_kwargs"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.batch_elements_kwargs">[docs]</a>  <span class="k">def</span> <span class="nf">batch_elements_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batching_kwargs</span></div>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.share_model_across_processes"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.share_model_across_processes">[docs]</a>  <span class="k">def</span> <span class="nf">share_model_across_processes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_large_model</span></div>

<div class="viewcode-block" id="HuggingFacePipelineModelHandler.get_metrics_namespace"><a class="viewcode-back" href="../../../../apache_beam.ml.inference.huggingface_inference.html#apache_beam.ml.inference.huggingface_inference.HuggingFacePipelineModelHandler.get_metrics_namespace">[docs]</a>  <span class="k">def</span> <span class="nf">get_metrics_namespace</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">       A namespace for metrics collected by the RunInference transform.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;BeamML_HuggingFacePipelineModelHandler&#39;</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>